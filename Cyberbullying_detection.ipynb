{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nightmare125/Cyberbullying-Detection/blob/main/Cyberbullying_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9GgOXWxdmGw"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f74mzVEdRi_O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUmAZ9pJTiM5"
      },
      "source": [
        "Load Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCzdrwUTtfe",
        "outputId": "35afa32b-5e89-4963-acda-ede54c606e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "Sr0lD_3k4BOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "3Hy42XdsVHMh"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/Cyberbullying Dataset/\"\n",
        "train_df = pd.read_csv(data_path + \"cyberbullying_tweets Training.csv\")\n",
        "val_df = pd.read_csv(data_path + \"cyberbullying_tweets Validating.csv\")\n",
        "test_df = pd.read_csv(data_path + \"cyberbullying_tweets Testing.csv\")\n",
        "collective_df = pd.read_csv(data_path + \"cyberbullying_tweets.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained tokenizer for text processing"
      ],
      "metadata": {
        "id": "rYubwFAe4E_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize and preprocess input text.\"\"\"\n",
        "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0tExhrLZ4ESw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzo9G3ksV5Uk"
      },
      "source": [
        "Custom dataset class for Cyberbullying detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "02vinG2GVxSL"
      },
      "outputs": [],
      "source": [
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = [preprocess_text(text) for text in texts]\n",
        "        self.labels = labels.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val.squeeze(0) for key, val in self.encodings[idx].items()}, torch.tensor(self.labels[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DApJupITXfrJ"
      },
      "source": [
        "Dataset Objects for Training, Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns = train_df.columns.str.strip()\n",
        "test_df.columns = test_df.columns.str.strip()\n",
        "val_df.columns = val_df.columns.str.strip()"
      ],
      "metadata": {
        "id": "pIXLhlrtG9vh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)  # Check the available columns\n",
        "print(test_df.columns)\n",
        "print(val_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dZmrrn9HoQK",
        "outputId": "50f87c14-1376-4fd2-ce39-0fdda79427bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['tweet_text', 'cyberbullying_type'], dtype='object')\n",
            "Index(['tweet_text', 'cyberbullying_type'], dtype='object')\n",
            "Index(['tweet_text', 'cyberbullying_type'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mcJ90XXWXfVa"
      },
      "outputs": [],
      "source": [
        "train_dataset = CyberbullyingDataset(train_df[\"tweet_text\"], train_df[\"cyberbullying_type\"])\n",
        "test_dataset = CyberbullyingDataset(test_df[\"tweet_text\"], test_df[\"cyberbullying_type\"])\n",
        "val_dataset = CyberbullyingDataset(val_df[\"tweet_text\"], val_df[\"cyberbullying_type\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkqh-aKIYYdi"
      },
      "source": [
        "Apply Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSV25YVbYbsr"
      },
      "outputs": [],
      "source": [
        "df[\"tweet_text\"] = df[\"tweet_text\"].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUOyO1ihYvM6"
      },
      "source": [
        "Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkCMD-M7YxAl"
      },
      "outputs": [],
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"tweet_text\"], df[\"label\"], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZr9-unMZSR6"
      },
      "source": [
        "Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cevfu6TUZR7J",
        "outputId": "fa51f9a0-439f-4e2f-a621-dd042c92a3f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6suDHPMHZij1"
      },
      "source": [
        "Tokenization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJGIATi5ZhUY"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(texts):\n",
        "  return tokenizer(list(texts), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings = tokenize_text(train_texts)\n",
        "val_encodings = tokenize_text(val_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D-KmRw_ax8l"
      },
      "source": [
        "Convert labels to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkdMyRK4a7Kj"
      },
      "outputs": [],
      "source": [
        "train_labels = torch.tensor(train_labels.tolist())\n",
        "val_labels = torch.tensor(val_labels.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyqB6iENbVUo"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NMcdaw_bUsq"
      },
      "outputs": [],
      "source": [
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJI7_AAkdTfc"
      },
      "source": [
        "Create Dataset Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "CXYP7yEWKPfE",
        "outputId": "27c20846-c834-45f0-8874-f5d30f54087e"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-23-3952010849b9>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-3952010849b9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    train_dataset=\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "train_dataset=\n",
        "test_dataset=\n",
        "val_dataset="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD1aZp0cdS88"
      },
      "outputs": [],
      "source": [
        "train_dataset = CyberbullyingDataset(train_encodings, train_labels)\n",
        "val_dataset = CyberbullyingDataset(val_encodings, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-oPZEvzeaQL"
      },
      "source": [
        "Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYLJwi2CeZ25"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GZ-V_vxetHT"
      },
      "source": [
        "Load Pretrained model for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BInxlwfhex7s",
        "outputId": "108961ab-e892-49c4-cd57-e8a87996391f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es8O0iB4esns"
      },
      "source": [
        "Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xKz0zcytfkcx",
        "outputId": "087e4ba3-9573-4e7c-f7b5-39883458f84a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gqMZvX8gB4h"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7vZjvxjrHh_",
        "outputId": "10a0476a-f42d-4553-d44b-217e7eeb8b35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-f1f27cedbb67>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  train_preds, train_truths, train_loss = [], [], 0.0\n",
        "  for batch in train_loader:\n",
        "    inputs, labels = batch\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(**inputs)\n",
        "    loss = loss_fn(outputs.logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss +=loss.item()\n",
        "    train_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "    train_truths.extend(labels.cpu().numpy())\n",
        "\n",
        "  train_acc = accuracy_score(train_truths, train_preds)\n",
        "  train_losses.append(train_loss / len(train_loader))\n",
        "  train_accuracies.append(train_acc)\n",
        "\n",
        "  model.eval()\n",
        "  val_preds, val_truths, val_loss = [], [], 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      inputs, labels = batch\n",
        "      inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(**inputs)\n",
        "      loss = loss_fn(outputs.logits, labels)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "      val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "      val_truths.extend(labels.cpu().numpy())\n",
        "\n",
        "  val_acc= accuracy_score(val_truths, val_preds)\n",
        "  val_losses.append(val_loss / len(val_loader))\n",
        "  val_accuracies.append(val_acc)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}: Train Acc= {train_acc:.4f}, Val Acc= {val_acc:.4f}, Train Loss= {train_losses[-1]:.4f}, Val Loss={val_losses[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qIezRCSprcM"
      },
      "source": [
        "Plot Accuracy and Loss Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg5DjP1TwYsu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, epochs + 1), val_accuracies, label='Val Accuracy', marker='s')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, epochs + 1), val_losses, label='Val Loss', marker='s')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBIxRKxDH89C"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEXdb6WIBPx"
      },
      "source": [
        "Test model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCKMQ0lIEGm"
      },
      "source": [
        "Validate model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO95XoCIvFUNE0TR5uTDWN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}